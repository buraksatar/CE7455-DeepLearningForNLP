{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"PyTorch-intro.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"1aG6DTtWokLY","colab_type":"text"},"source":["# Introduction to PyTorch\n","\n","Before reading this introduction you should know a bit of:\n","1. Python - look at [official tutorial](https://docs.python.org/3/tutorial/)\n","2. Linear Algebra and Matrices - look at [Coursera tutorial](https://www.coursera.org/learn/linear-algebra-machine-learning) and/or book [Introduction to Applied Linear Algebra](http://vmls-book.stanford.edu/vmls.pdf)\n","\n","\n","\n","<hr>\n","\n","\n","From official NumPy page we could read that\n","\n","```\n","PyTorch is a Python-based scientific computing package targeted at two sets of audiences:\n","\n","* A replacement for NumPy to use the power of GPUs\n","* a deep learning research platform that provides maximum flexibility and speed\n","```\n","\n","Contrary to NumPy, PyTorch was designed mostly to work on **GPU**. PyTorch represents n-dimensional array object as  `Tensor`. To install PyTorch library, go to [link](https://pytorch.org/get-started/locally/). There are also very good tutorials:\n","* [Official PyTorch tutorials](https://pytorch.org/tutorials/)\n","* [Deep Learning for Natural Language Processing with Pytorch](https://github.com/rguthrie3/DeepLearningForNLPInPytorch/blob/master/Deep%20Learning%20for%20Natural%20Language%20Processing%20with%20Pytorch.ipynb) \n","\n","Here we want give you a quick crash course of using PyTorch library, especially Tensor object. "]},{"cell_type":"markdown","metadata":{"id":"wk2YydsWokLb","colab_type":"text"},"source":["## Basics: creating a PyTorch tensor\n","\n","Important notes:\n","* all items in PyTorch array (a.k.a. `Tensor`) cantain only one data type e.g. `int8`, `float32`, ... ([all datatypes](https://pytorch.org/docs/stable/tensors.html))"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-10-13T18:13:36.933429Z","start_time":"2019-10-13T18:13:36.703426Z"},"id":"ZeKrtCJiokLc","colab_type":"code","outputId":"fb7ffe8d-ea99-4e85-a3d8-1d2bdada17d8","colab":{}},"source":["import torch \n","\n","print(\"1d Tensor from Python list (with `int32` type)\")\n","list1d = [0, 1, 2, 3, 4, 5, 6, 7]\n","tensor1d = torch.tensor(list1d, dtype=torch.int32) \n","print(tensor1d) # print tensor\n","print(tensor1d.size()) # print tensor shape\n","print()\n","\n","print(\"2d Tensor from Python list of lists  (with `float32` type)\")\n","list2d = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]\n","tensor2d = torch.tensor(list2d, dtype=torch.float32)\n","print(tensor2d) # print tensor\n","print(tensor2d.size()) # print tensor shape\n","print()\n","\n","print(\"1d random tensor (with `float32` type)\")\n","tensor1d_random = torch.rand(8, 2, dtype=torch.float32)\n","print(tensor1d_random) # print tensor\n","print(tensor1d_random.size()) # print tensor shape\n","print()\n","\n","print(\"1d tensor with uniform distribution\")\n","tensor1d_uniform = torch.FloatTensor(16, 1).uniform_(-10, 10)\n","print(tensor1d_uniform) # print tensor\n","print(tensor1d_uniform.size()) # print tensor shape\n","print()\n","\n","print(\"1d tensor based on linearly spaced vector\")\n","tensor1d_linspace = torch.linspace(0, 7, steps=8, dtype=torch.float32)\n","print(tensor1d_linspace) # print tensor\n","print(tensor1d_linspace.size()) # print tensor shape\n","print()\n","\n","print(\"1d tensor based on `arange` mechanism\")\n","tensor1d_arange = torch.arange(0, 10, 3)\n","print(tensor1d_arange) # print tensor\n","print(tensor1d_arange.size()) # print tensor shape\n","print()\n","\n","print(\"2d zeros tensor\")\n","torch2d_zeros = torch.zeros([2, 4])\n","print(torch2d_zeros) # print tensor\n","print(torch2d_zeros.size()) # print tensor shape\n","print()\n","\n","print(\"2d ones tensor\")\n","torch2d_ones = torch.ones([2, 4])\n","print(torch2d_ones) # print tensor\n","print(torch2d_ones.size()) # print tensor shape\n","print()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1d Tensor from Python list (with `int32` type)\n","tensor([0, 1, 2, 3, 4, 5, 6, 7], dtype=torch.int32)\n","torch.Size([8])\n","\n","2d Tensor from Python list of lists  (with `float32` type)\n","tensor([[ 0.,  1.],\n","        [ 2.,  3.],\n","        [ 4.,  5.],\n","        [ 6.,  7.],\n","        [ 8.,  9.],\n","        [10., 11.],\n","        [12., 13.],\n","        [14., 15.]])\n","torch.Size([8, 2])\n","\n","1d random tensor (with `float32` type)\n","tensor([[0.2339, 0.1720],\n","        [0.7893, 0.9454],\n","        [0.3148, 0.3166],\n","        [0.7262, 0.8609],\n","        [0.0912, 0.0767],\n","        [0.5113, 0.8339],\n","        [0.7812, 0.9421],\n","        [0.1299, 0.1272]])\n","torch.Size([8, 2])\n","\n","1d tensor with uniform distribution\n","tensor([[-6.8103],\n","        [ 0.8485],\n","        [ 6.5259],\n","        [ 2.5182],\n","        [ 0.6380],\n","        [-3.1469],\n","        [-3.4691],\n","        [ 4.6706],\n","        [ 2.0206],\n","        [-8.8526],\n","        [-3.8986],\n","        [ 0.9371],\n","        [-9.4456],\n","        [-8.4750],\n","        [-1.1130],\n","        [ 3.2142]])\n","torch.Size([16, 1])\n","\n","1d tensor based on linearly spaced vector\n","tensor([0., 1., 2., 3., 4., 5., 6., 7.])\n","torch.Size([8])\n","\n","1d tensor based on `arange` mechanism\n","tensor([0, 3, 6, 9])\n","torch.Size([4])\n","\n","2d zeros tensor\n","tensor([[0., 0., 0., 0.],\n","        [0., 0., 0., 0.]])\n","torch.Size([2, 4])\n","\n","2d ones tensor\n","tensor([[1., 1., 1., 1.],\n","        [1., 1., 1., 1.]])\n","torch.Size([2, 4])\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r0xD9G7TokLh","colab_type":"text"},"source":["## Basics: extracting specific values from tensors\n","\n","Important notes:\n","* tensor can be indexed using the standard Python x[obj] syntax, wherea x is the array and obj the selection"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-10-13T18:13:36.942523Z","start_time":"2019-10-13T18:13:36.935148Z"},"id":"SLhk6f7uokLi","colab_type":"code","outputId":"d828dc61-1e0d-4568-dc1d-dbf7976325e8","colab":{}},"source":["print(\"Get specific element\")\n","print(tensor2d[1,1])\n","print()\n","\n","print(\"The basic slice syntax is i:j:k where i is the starting index, j is the stopping index, and k is the step\")\n","print(tensor1d[0:6:2])\n","print()\n","\n","print(\"Extract only one dimension from multidimensional ndarray\") \n","print(tensor2d[:, 0])\n","print()\n","\n","print(\"Boolean array indexing\") \n","print(tensor1d[([True, False, True, False, True, False, True, False])])\n","print()\n","\n","print(\"Using condition statement for indexing array\") \n","print(tensor1d[(tensor1d % 2 == 0)]) \n","print()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Get specific element\n","tensor(3.)\n","\n","The basic slice syntax is i:j:k where i is the starting index, j is the stopping index, and k is the step\n","tensor([0, 2, 4], dtype=torch.int32)\n","\n","Extract only one dimension from multidimensional ndarray\n","tensor([ 0.,  2.,  4.,  6.,  8., 10., 12., 14.])\n","\n","Boolean array indexing\n","tensor([0, 2, 4, 6], dtype=torch.int32)\n","\n","Using condition statement for indexing array\n","tensor([0, 2, 4, 6], dtype=torch.int32)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LxsaRlGkokLk","colab_type":"text"},"source":["## Basics: sum, min, max, mean, reshape "]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-10-13T18:13:36.959263Z","start_time":"2019-10-13T18:13:36.944221Z"},"id":"fKwA8BISokLl","colab_type":"code","outputId":"216dcc03-23a8-4371-e58b-e943d167cacf","colab":{}},"source":["print(\"represent `not a number value`\")\n","print(torch.tensor(float('nan')))\n","print()\n","\n","print(\"represent `infinite`\")\n","print(torch.tensor(float('Inf')))\n","print()\n","\n","print(\"calculate mean, max and min in tensor\")\n","print(\"sum \", tensor2d.sum())\n","print(\"max \", tensor2d.max())\n","print(\"min \", tensor2d.min())\n","print(\"mean \", tensor2d.mean())\n","print()\n","\n","print(\"calculate max on different axis\")\n","print(\"column max: \", tensor2d.max(dim=0)[0])\n","print(\"row max: \", tensor2d.max(dim=1)[0])\n","print()\n","\n","print(\"reshape 2d tensor\")\n","print(tensor2d.view(4, 4))\n","print(tensor2d.size(), tensor2d.view(4, 4).size())\n","print()\n","\n","print(\"reshape 2d tensor (second way)\")\n","print(tensor2d.view(4, -1)) # the second dimention is adjusted to size of the matrix\n","print(tensor2d.size(), tensor2d.view(4, -1).size())\n","print()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["represent `not a number value`\n","tensor(nan)\n","\n","represent `infinite`\n","tensor(inf)\n","\n","calculate mean, max and min in tensor\n","sum  tensor(120.)\n","max  tensor(15.)\n","min  tensor(0.)\n","mean  tensor(7.5000)\n","\n","calculate max on different axis\n","column max:  tensor([14., 15.])\n","row max:  tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15.])\n","\n","reshape 2d tensor\n","tensor([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [12., 13., 14., 15.]])\n","torch.Size([8, 2]) torch.Size([4, 4])\n","\n","reshape 2d tensor (second way)\n","tensor([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [12., 13., 14., 15.]])\n","torch.Size([8, 2]) torch.Size([4, 4])\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MfFeOqbwokLn","colab_type":"text"},"source":["## Basics: tensor math"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-10-13T18:13:36.979309Z","start_time":"2019-10-13T18:13:36.961184Z"},"id":"u9qNagd4okLo","colab_type":"code","outputId":"af6f17cc-b86a-47f6-ca9e-ffadf4a005dc","colab":{}},"source":["print(\"Initialize tensor, x1\")\n","x1 = torch.ones([3, 3], dtype=torch.float32)\n","x1[:, 0] = torch.tensor([2., 3., 4.])\n","print(x1)\n","print()\n","\n","print(\"Transpose x1 tensor\")\n","print(x1.transpose(0, 1))\n","print()\n","\n","print(\"Multiply by scalar, x2=x1*3\")\n","x2 = x1*3.\n","print(x2)\n","print()\n","\n","print(\"Element-wise sum, x1+x2\")\n","print(x1+x2)\n","print()\n","\n","print(\"Element-wise subtract, x1-x2\")\n","print(x1-x2)\n","print()\n","\n","print(\"Element-wise product, x1*x2\")\n","print(x1*x2)\n","print()\n","\n","print(\"Element-wise divide, x1/x2\")\n","print(x1/x2)\n","print()\n","\n","print(\"Element-wise power, x2^2\")\n","print(torch.pow(x2, 2))\n","print()\n","\n","print(\"Element-wise square root, sqrt(x2)\")\n","print(torch.sqrt(x2))\n","print()\n","\n","print(\"Matrix multiplication, x1*x2\")\n","print(x1.mm(x2))\n","print()\n","\n","print(\"Vector multiplication, x1*x2[0]\")\n","print(x1.mm(x2[0].view([-1, 1])).view(-1))\n","print()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialize tensor, x1\n","tensor([[2., 1., 1.],\n","        [3., 1., 1.],\n","        [4., 1., 1.]])\n","\n","Transpose x1 tensor\n","tensor([[2., 3., 4.],\n","        [1., 1., 1.],\n","        [1., 1., 1.]])\n","\n","Multiply by scalar, x2=x1*3\n","tensor([[ 6.,  3.,  3.],\n","        [ 9.,  3.,  3.],\n","        [12.,  3.,  3.]])\n","\n","Element-wise sum, x1+x2\n","tensor([[ 8.,  4.,  4.],\n","        [12.,  4.,  4.],\n","        [16.,  4.,  4.]])\n","\n","Element-wise subtract, x1-x2\n","tensor([[-4., -2., -2.],\n","        [-6., -2., -2.],\n","        [-8., -2., -2.]])\n","\n","Element-wise product, x1*x2\n","tensor([[12.,  3.,  3.],\n","        [27.,  3.,  3.],\n","        [48.,  3.,  3.]])\n","\n","Element-wise divide, x1/x2\n","tensor([[0.3333, 0.3333, 0.3333],\n","        [0.3333, 0.3333, 0.3333],\n","        [0.3333, 0.3333, 0.3333]])\n","\n","Element-wise power, x2^2\n","tensor([[ 36.,   9.,   9.],\n","        [ 81.,   9.,   9.],\n","        [144.,   9.,   9.]])\n","\n","Element-wise square root, sqrt(x2)\n","tensor([[2.4495, 1.7321, 1.7321],\n","        [3.0000, 1.7321, 1.7321],\n","        [3.4641, 1.7321, 1.7321]])\n","\n","Matrix multiplication, x1*x2\n","tensor([[33., 12., 12.],\n","        [39., 15., 15.],\n","        [45., 18., 18.]])\n","\n","Vector multiplication, x1*x2[0]\n","tensor([18., 24., 30.])\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2019-10-13T18:08:48.844141Z","start_time":"2019-10-13T18:08:48.839846Z"},"id":"m-TIC6l7okLq","colab_type":"text"},"source":["## Advanced: broadcasting\n","\n","In short, if a PyTorch operation supports broadcast, then its Tensor arguments can be automatically expanded to be of equal sizes (see more at [this link](https://pytorch.org/docs/stable/notes/broadcasting.html))"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-10-13T18:13:36.990697Z","start_time":"2019-10-13T18:13:36.981482Z"},"id":"0Ox25KQSokLr","colab_type":"code","outputId":"838b2457-843e-45b7-efa5-a109eef4aadc","colab":{}},"source":["print(\"Add vector x2 to each row of matrix x1 using broadcasting mechanism\")\n","x1 = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]])\n","x2 = torch.tensor([1, 3])\n","print(x1+x2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Add vector x2 to each row of matrix x1 using broadcasting mechanism\n","tensor([[ 1,  4],\n","        [ 3,  6],\n","        [ 5,  8],\n","        [ 7, 10],\n","        [ 9, 12],\n","        [11, 14],\n","        [13, 16],\n","        [15, 18]])\n"],"name":"stdout"}]}]}